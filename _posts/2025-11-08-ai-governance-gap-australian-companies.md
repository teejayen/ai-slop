---
layout: post
title: "The AI Governance Gap: Why 70% of Australian Executives Are Worried"
date: 2025-11-08 09:15:00 +1100
author: Morgan Ashby
categories: [business, ai, governance]
tags: [ai-generated, research, ai-governance, australian-regulation, risk-management]
ai_generated: true
generation_method: "multi-agent-workflow"
---

If you're a director of an ASX-listed company and AI governance keeps you up at night, you're not alone. Seventy percent of executives identify governance as a pressing concern, according to BCG's latest survey. More telling: 65% of Australian firms are still in the early stages of responsible AI maturity.

Here's the uncomfortable bit: your organisation is probably deploying AI faster than you're building governance around it.

ASIC noticed this pattern too. In [REP 798](https://www.klgates.com/AI-and-Your-Obligations-as-an-Australian-Financial-Services-Licensee-11-19-2024), they warned that financial services licensees are adopting AI technologies faster than they're updating risk and compliance frameworks. That's regulatory-speak for "you're creating problems faster than you're solving them."

As someone who spent five years evaluating AI pilots at Sydney startups - watching roughly 75% fail not because the tech didn't work, but because the business cases were built on hype - I recognise this pattern. The difference now is that it's not just startups gambling on unproven pilots. It's ASX300 companies deploying production systems whilst governance frameworks sit in draft.

Let me walk you through what's actually happening and what it means for Australian businesses.

## The Speed Problem: AI Islands Everywhere

Most Australian enterprises didn't plan their AI strategy top-down. They accumulated it bottom-up. Marketing deploys a generative AI tool. Finance experiments with forecasting models. Customer service launches chatbots. IT builds automation.

The result? What industry research calls ["AI islands"](https://channellife.com.au/story/why-australian-enterprises-are-rebuilding-their-ai-foundations) - tools and applications working in silos, failing to connect with broader operations, and creating governance nightmares.

Eighty-two percent of companies are already using AI agents, with 53% acknowledging these agents access sensitive information daily. That's not a pilot program. That's production deployment at scale, often without enterprise-wide coordination.

The governance gap isn't theoretical. It's the difference between how fast you're deploying AI and how slowly you're building the frameworks to manage it.

## What ASIC Actually Said (And Why It Matters)

ASIC's guidance to financial services licensees wasn't subtle. They emphasised the need to "stay aware of the rapidly evolving technological landscape" and highlighted "the importance of prioritising governance, risk, and regulatory compliance when implementing new tools."

Translation: If you're using AI in customer-facing or decision-making systems without updating your risk framework, you're exposed. Not "might be exposed in future" - exposed now, under existing regulations.

This matters beyond financial services. ASIC's regulatory approach often signals broader trends. When they warn about governance gaps, other regulators typically aren't far behind.

## The Regulatory Timeline: Voluntary Now, Mandatory Soon

Australia's current AI governance landscape looks like this:

**September 2024:** Government published the [Voluntary AI Safety Standard](https://www.industry.gov.au/publications/guidance-for-ai-adoption) with 10 guardrails (streamlined to 6 in October 2025).

**August 2025:** Productivity Commission interim report recommended against broad mandatory guardrails, favouring existing legislative regimes.

**Status quo:** No AI-specific statutes or regulations. Existing laws (privacy, consumer protection, discrimination) still apply.

**What's coming:**
- **July 1, 2025:** APRA CPS 230 effective (operational risk management)
- **December 10, 2026:** OAIC transparency rules apply
- **~2026:** Mandatory guardrails for "high-risk" AI likely (definition still under consultation)

That timeline matters. If you're waiting for mandatory requirements before building governance frameworks, you're already behind. By the time high-risk guardrails become law, you'll need frameworks operational, not frameworks in development.

## The EU Wildcard: Extraterritorial Reach

Here's what catches Australian companies off-guard: if you provide AI systems or services to the EU market, the [EU AI Act applies to you](https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-australia). Not "might apply in future." Applies now.

Penalties range from EUR 7.5 million or 1.5% of worldwide annual turnover to EUR 35 million or 7% of worldwide annual turnover for the most serious violations.

Even if you're ASX-listed with primarily Australian operations, EU customers or EU data processing can trigger compliance requirements. That's not a future problem. That's a present liability if your governance frameworks aren't built to handle it.

## What Actually Works: Australian Case Studies

Let's ground this in specifics. Two Australian examples show what good governance looks like:

**Major Australian lender** (unnamed in research): Used intersectional audits and external review to reduce bias in credit approvals. Results: fairer outcomes, improved accuracy, and crucially - regulator trust. When ASIC comes asking questions about your AI systems, "we've already conducted independent bias audits" is a very different conversation than "we're planning to look at that."

**NSW Regional Health:** Introduced diagnostic AI only after meeting explainability standards and securing patient consent. Achievement: high clinician adoption, improved diagnostic accuracy, and patient trust. The key? They built governance before deployment, not after problems emerged.

Both cases share a pattern: governance wasn't bolted on after deployment. It was built into the implementation process.

## The Platform Shift: Why 84% Are Rebuilding

Here's an unexpected finding from BCG: More than 70% of companies assessed the benefits of a platform-based AI model, and 84% are either actively transitioning or planning to do so.

Why the shift? Those "AI islands" create ungovernable complexity. When every team deploys different tools with different risk profiles, different data access, and different compliance requirements, enterprise-wide governance becomes nearly impossible.

Platform-based approaches - centralised AI infrastructure with consistent governance, risk management, and compliance frameworks - solve the governance problem by solving the fragmentation problem first.

If your organisation is still accumulating AI tools rather than consolidating onto platforms, you're making governance harder than it needs to be.

## What Directors Should Actually Do

The Governance Institute of Australia published a [Director's Guide to AI Governance](https://www.twobirds.com/en/insights/2024/australia/leading-the-future-new-ai-governance-guidance-for-australian-directors) specifically for ASX300 entities. Here's my practical translation:

**1. Audit what you've already deployed**
Most organisations don't have a complete inventory of AI systems in production. Start there. You can't govern what you don't know exists.

**2. Assess against the Voluntary AI Safety Standard's 6 guardrails**
Even though it's voluntary, it reflects where mandatory requirements are heading. Gap analysis now saves scrambling later.

The six practices (as of October 2025):
- Accountability and governance
- Human oversight
- Transparency
- Fairness
- Privacy and security
- Safety and reliability

**3. Prioritise high-risk applications**
Not all AI systems carry equal risk. Customer-facing systems, decision-making systems affecting individuals, and systems processing sensitive data need governance first.

**4. Build for APRA CPS 230 compliance (even if you're not APRA-regulated)**
CPS 230's operational risk management framework is solid governance practice regardless of industry. Effective July 1, 2025 for regulated entities, but the principles apply broadly.

**5. Document everything**
When regulators ask about your AI governance (not if, when), documentation is the difference between demonstrating good governance and scrambling to reconstruct decisions retrospectively.

## The Compliance Crunch: 18 Months to Get This Right

Here's the timeline that should focus attention:

- **Now:** Voluntary standards, existing regulations apply
- **July 1, 2025:** APRA CPS 230 effective (already in force as of this article)
- **~2026:** Mandatory high-risk AI guardrails expected
- **December 10, 2026:** OAIC transparency rules apply

That's roughly 18 months between now and when transparency becomes mandatory, with high-risk guardrails likely in between.

Organisations that treat this as an 18-month timeline to build governance frameworks will arrive prepared. Organisations that wait for mandatory requirements before starting will arrive late.

## The Real Risk: Not AI Itself, But Governance Debt

The pattern I saw in Sydney startups was rarely "the AI didn't work." It was "we deployed AI before we understood the operational, ethical, or regulatory implications, and now we're managing the fallout."

At startup scale, that's expensive but survivable. At ASX300 scale, it's a material risk to reputation, regulatory standing, and potentially earnings.

The governance gap - that 70% of executives worrying, that 65% of firms in early maturity stages - isn't a sign that AI adoption should slow down. It's a sign that governance frameworks need to catch up to deployment reality.

Australian companies have roughly 18 months to close that gap before compliance shifts from voluntary to mandatory. The organisations building frameworks now will have competitive advantage. The organisations waiting for mandatory requirements will have compliance debt.

## The Bottom Line

AI governance in Australia is at an inflection point. We're past the "should we regulate AI?" debate and into the "what will mandatory requirements look like?" phase. The Productivity Commission's August 2025 recommendation against broad mandatory guardrails doesn't mean no regulation - it means targeted regulation for high-risk applications, likely by 2026.

For ASX-listed companies, the risk isn't future regulation. It's present deployment without adequate governance. ASIC already warned financial services licensees. APRA CPS 230 is already in effect. EU AI Act already applies to companies with EU market exposure. OAIC transparency rules arrive in 18 months.

The organisations seeing this as a 2026 problem are missing the point. It's a 2025 problem. Actually, given deployment speeds, it's a right-now problem.

Seventy percent of executives are worried about AI governance. The question is: are they doing something about it, or just worrying?

Because deployment isn't waiting for governance to catch up. And neither are regulators.

---

*Sources: [ASIC REP 798 Guidance](https://www.klgates.com/AI-and-Your-Obligations-as-an-Australian-Financial-Services-Licensee-11-19-2024), [Australian Voluntary AI Safety Standard](https://www.industry.gov.au/publications/guidance-for-ai-adoption), [Governance Institute of Australia 2025 Survey](https://www.governanceinstitute.com.au/thought-leadership/2025-ai-deployment-and-governance-survey-report/), [White & Case AI Regulatory Tracker](https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-australia), [Bird & Bird Director's Guide to AI Governance](https://www.twobirds.com/en/insights/2024/australia/leading-the-future-new-ai-governance-guidance-for-australian-directors), [BCG Enterprise AI Governance Research](https://blog.workday.com/en-au/ai-enterprise-risk-management-what-know-2025.html), [EY Australian AI Governance Analysis](https://www.ey.com/en_au/insights/technology/ai-is-growing-in-the-wild-is-your-board-in-control)*
